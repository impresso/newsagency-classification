{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import zipfile\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_dict = {\"AFP1\": \"AFP\", \n",
    "           \"AFP2\": \"AFP\", \n",
    "           \"ANP\": \"ANP\", \n",
    "           \"ANSA\": \"ANSA\", \n",
    "           \"AP1\": \"AP\", \n",
    "           \"AP2\": \"AP\", \n",
    "           \"APA\": \"APA\", \n",
    "           \"Associated_Press\": \"Associated Press\", \n",
    "           \"ATS1\": \"ATS\",\n",
    "           \"Belga\": \"Belga\", \n",
    "           \"BTA\": \"BTA\", \n",
    "           \"CTK\": \"CTK\", \n",
    "           \"DDP-DAPD\": \"DDP-DAPD\", \n",
    "           \"DNB\": \"DNB\",\n",
    "           \"Domei\": \"Domei\", \n",
    "           \"DPA\": \"DPA\", \n",
    "           \"Europapress\": \"Europapress\", \n",
    "           \"Extel\": \"Extel\", \n",
    "           \"Havas\": \"Havas\",\n",
    "           \"Havasagentur\": \"Havas\",\n",
    "           \"Interfax\": \"Interfax\",\n",
    "           \"PAP\": \"PAP\",\n",
    "           \"Reuter\": \"Reuters\",\n",
    "           \"Reuters\": \"Reuters\",\n",
    "           \"reutersche\": \"Reuters\",\n",
    "           \"SPK\": \"SPK\",\n",
    "           \"Stefani\": \"Stefani\",\n",
    "           \"Tanjug\": \"Tanjug\",\n",
    "           \"TASS\": \"TASS\",\n",
    "           \"Telunion\": \"Telunion\",\n",
    "           \"TT-Sweden\": \"TT\",\n",
    "           \"UPI\": \"UP-UPI\",\n",
    "           \"Wolff\": \"Wolff\"\n",
    "          \n",
    "          }\n",
    "\n",
    "\n",
    "#na_dict = {\"Belga\":\"Belga\"}\n",
    "\n",
    "#all columns but \"type\" (empty) and last one (only title, indicating size of the collection)\n",
    "cols = ['uid', 'language', 'title', 'size', 'country', 'newspaper',\n",
    "       'issue', 'pages', 'nb_pages', 'relevance', 'year', 'is_on_front',\n",
    "       'date', 'persons_mentioned', 'locations_mentioned', 'content',\n",
    "       'access_right', 'content_provider', 'is_content_available',\n",
    "       'collections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build full collection in one dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for na_collection, na_name in na_dict.items():\n",
    "    #import next news agency content\n",
    "    path = \"zips/\" + na_collection + \".zip\"\n",
    "    temp = pd.read_csv(path, compression=\"zip\", sep=\";\", usecols=cols)\n",
    "    #save name of news agency\n",
    "    temp[\"newsagency\"] = na_name\n",
    "    #add to rest\n",
    "    df = pd.concat([df,temp])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation of \"Mixed\" collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nas = {\"afpreuters\": [\"AFP\", \"Reuters\"], \n",
    "             \"afpreuter\": [\"AFP\", \"Reuters\"], \n",
    "             \"atsafp\": [\"ATS\", \"AFP\"], \n",
    "             \"atsreuters\": [\"ATS\", \"Reuters\"], \n",
    "             \"atsreuter\": [\"ATS\", \"Reuters\"], \n",
    "             \"atsjafp\": [\"ATS\", \"AFP\"], \n",
    "             \"atsap\": [\"ATS\", \"AP\"], \n",
    "             \"aplddp\": [\"AP\", \"DDP\"], \n",
    "             \"aplafp\": [\"AP\", \"AFP\"],\n",
    "             \"afplap\": [\"AFP\", \"AP\"], \n",
    "             \"dpalafp\": [\"DPA\", \"AFP\"], \n",
    "             \"atsjreuter\": [\"ATS\", \"Reuters\"], \n",
    "             \"atsfafp\": [\"ATS\", \"AFP\"], \n",
    "             \"ddplap\": [\"DDP\", \"AP\"], \n",
    "             \"aplsda\": [\"AP\", \"ATS\"], \n",
    "             \"aplddp\": [\"AP\", \"DDP\"], \n",
    "             \"sdalafp\": [\"ATS\", \"AFP\"], \n",
    "             \"atsjred\": [\"ATS\"], \n",
    "             \"atsred\": [\"ATS\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = len(df)\n",
    "\n",
    "#import Mixed.zip file\n",
    "mixed = pd.read_csv(\"zips/Mixed.zip\", compression=\"zip\", sep=\";\", usecols=cols)\n",
    "mixed = mixed[mixed[\"content\"].notna()]\n",
    "\n",
    "#normalize content column in a copy of Mixed\n",
    "mixed_no_acc = mixed.copy()\n",
    "mixed_no_acc[\"content\"] = mixed_no_acc[\"content\"].apply(lambda x: unidecode(x))\n",
    "\n",
    "count = 0\n",
    "for mixed_word, mixed_na_list in mixed_nas.items():\n",
    "    #get all entries with the mixed_word in it\n",
    "    temp = mixed[mixed_no_acc[\"content\"].str.contains(mixed_word, case=False, na=False)]\n",
    "    \n",
    "    #first store mixed_na_list in a new column, then create one row per na entry (via explode)\n",
    "    temp =  temp.assign(newsagency = [mixed_na_list] * len(temp))\n",
    "    temp = temp.explode(\"newsagency\", ignore_index=True)\n",
    "    count += len(temp)\n",
    "    \n",
    "    #concatenate to existing df\n",
    "    df = pd.concat([df,temp])\n",
    "\n",
    "#delete duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"#articles in Mixed.zip: {len(mixed)}, #rows stored (with duplicates): {count},\\n\\\n",
    "#rows stored (without duplicates): {len(df)-len_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pauline\n",
    "def get_decade(year):\n",
    "    return int((year//10)*10)\n",
    "\n",
    "#add decade column\n",
    "df[\"decade\"] = df[\"year\"].apply(lambda x: int(get_decade(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_content = df[df[\"content\"].isnull()]\n",
    "print(f\"#articles without content: {len(no_content)}, percentage: {len(no_content)/len(df)}\")\n",
    "no_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"content\"].notna()]\n",
    "print(\"After deleting articles without content:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsagencies = df[[\"newsagency\", \"uid\"]].groupby(\"newsagency\").count().rename(columns={\"uid\":\"has_content\"})\n",
    "#join with no_content dataframe which is also grouped by newsagencies\n",
    "Newsagencies = Newsagencies.join(no_content[[\"newsagency\", \"uid\"]].groupby(\"newsagency\").count().rename(columns={\"uid\":\"no_content\"}))\n",
    "Newsagencies = Newsagencies.fillna(0)\n",
    "\n",
    "#column: percentage of entries without content\n",
    "Newsagencies[\"perc_no_content\"] = Newsagencies.apply(lambda x: x.no_content/(x.has_content + x.no_content), axis=1)\n",
    "Newsagencies[\"perc_no_content\"].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsagencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several Newsagency mentions per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df = df[[\"uid\", \"newsagency\"]]\n",
    "na_df_grouped = na_df.groupby(\"uid\")[\"newsagency\"].apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_unique = len(df.groupby(\"uid\").count())\n",
    "print(f\"unique: {unique}, percentage of articles only contained in one collection: {unique/len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_per_decade = pd.pivot_table(df, index=\"decade\", columns=\"newsagency\", values=\"uid\", aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_per_decade.plot.line(figsize=(16,8), title=\"News Agency content per decade and agency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_per_decade.iloc[:16].plot.line(figsize=(16,8), title=\"News Agency content per decade and agency, 1750-1920\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_per_decade.plot.bar(stacked=True, figsize=(16,8), title=\"News Agency content per decade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
