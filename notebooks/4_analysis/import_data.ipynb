{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data \n",
    "from Dask-Batches and save it as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../na_mentions_final/\"\n",
    "data_folders = sorted([folder for folder in os.listdir(data_path)])\n",
    "output_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Folder 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_4.jsonl.bz2', '7_5.jsonl.bz2', '5_2.jsonl.bz2', '3_4.jsonl.bz2', '1_7.jsonl.bz2', '8_5.jsonl.bz2', '1_8.jsonl.bz2', '4_4.jsonl.bz2', '6_1.jsonl.bz2', '4_6.jsonl.bz2', '0_0.jsonl.bz2', '3_2.jsonl.bz2', '1_5.jsonl.bz2', '4_5.jsonl.bz2', '6_8.jsonl.bz2', '5_8.jsonl.bz2', '0_7.jsonl.bz2', '9_2.jsonl.bz2', '5_4.jsonl.bz2', '8_2.jsonl.bz2', '3_6.jsonl.bz2', '0_4.jsonl.bz2', '2_2.jsonl.bz2', '2_0.jsonl.bz2', '5_3.jsonl.bz2', '4_2.jsonl.bz2', '0_9.jsonl.bz2', '3_5.jsonl.bz2', '4_8.jsonl.bz2', '6_3.jsonl.bz2', '2_5.jsonl.bz2', '1_3.jsonl.bz2', '8_3.jsonl.bz2', '4_3.jsonl.bz2', '8_7.jsonl.bz2', '5_0.jsonl.bz2', '9_4.jsonl.bz2', '8_8.jsonl.bz2', '2_7.jsonl.bz2', '3_3.jsonl.bz2', '0_3.jsonl.bz2', '1_9.jsonl.bz2', '3_0.jsonl.bz2', '2_8.jsonl.bz2', '0_8.jsonl.bz2', '2_4.jsonl.bz2', '1_2.jsonl.bz2', '9_9.jsonl.bz2', '2_9.jsonl.bz2', '5_6.jsonl.bz2', '4_9.jsonl.bz2', '9_8.jsonl.bz2', '0_2.jsonl.bz2', '1_0.jsonl.bz2', '2_6.jsonl.bz2', '9_7.jsonl.bz2', '9_1.jsonl.bz2', '8_4.jsonl.bz2', '7_6.jsonl.bz2', '9_5.jsonl.bz2', '9_0.jsonl.bz2', '2_3.jsonl.bz2', '6_4.jsonl.bz2', '6_9.jsonl.bz2', '0_5.jsonl.bz2', '7_3.jsonl.bz2', '7_0.jsonl.bz2', '6_7.jsonl.bz2', '6_6.jsonl.bz2', '3_7.jsonl.bz2', '6_5.jsonl.bz2', '3_1.jsonl.bz2', '2_1.jsonl.bz2', '1_6.jsonl.bz2', '0_1.jsonl.bz2', '7_7.jsonl.bz2', '7_8.jsonl.bz2', '7_9.jsonl.bz2', '6_2.jsonl.bz2', '8_0.jsonl.bz2', '7_2.jsonl.bz2', '3_8.jsonl.bz2', '5_9.jsonl.bz2', '5_7.jsonl.bz2', '5_1.jsonl.bz2', '3_9.jsonl.bz2', '8_6.jsonl.bz2', '7_4.jsonl.bz2', '9_6.jsonl.bz2', '8_9.jsonl.bz2', '4_1.jsonl.bz2', '7_1.jsonl.bz2', '4_7.jsonl.bz2', '9_3.jsonl.bz2', '8_1.jsonl.bz2', '1_1.jsonl.bz2', '0_6.jsonl.bz2', '6_0.jsonl.bz2', '5_5.jsonl.bz2', '4_0.jsonl.bz2']\n",
      "2902\n",
      "2878\n",
      "3048\n",
      "2828\n",
      "3112\n",
      "3047\n",
      "2997\n",
      "3000\n",
      "3015\n",
      "2841\n",
      "2758\n",
      "2925\n",
      "2871\n",
      "2803\n",
      "2934\n",
      "2865\n",
      "3142\n",
      "3011\n",
      "2902\n",
      "3033\n",
      "3031\n",
      "2992\n",
      "3149\n",
      "3081\n",
      "3153\n",
      "3111\n",
      "2983\n",
      "3132\n",
      "3037\n",
      "2702\n",
      "2941\n",
      "2963\n",
      "2728\n",
      "2969\n",
      "2987\n",
      "2832\n",
      "2866\n",
      "3110\n",
      "2820\n",
      "2849\n",
      "2890\n",
      "3019\n",
      "2801\n",
      "2934\n",
      "2826\n",
      "2758\n",
      "3030\n",
      "2884\n",
      "2776\n",
      "2786\n",
      "2825\n",
      "2938\n",
      "2991\n",
      "2960\n",
      "2955\n",
      "2812\n",
      "3031\n",
      "2869\n",
      "3066\n",
      "2799\n",
      "2915\n",
      "3102\n",
      "2798\n",
      "3061\n",
      "2868\n",
      "2807\n",
      "2822\n",
      "2841\n",
      "2984\n",
      "2945\n",
      "2768\n",
      "2897\n",
      "2946\n",
      "2860\n",
      "2920\n",
      "3034\n",
      "2863\n",
      "2970\n",
      "2945\n",
      "2837\n",
      "3020\n",
      "2831\n",
      "2903\n",
      "2875\n",
      "2900\n",
      "2895\n",
      "2671\n",
      "2988\n",
      "2841\n",
      "2856\n",
      "2724\n",
      "2977\n",
      "3112\n",
      "2979\n",
      "3060\n",
      "2912\n",
      "2949\n",
      "2929\n",
      "2863\n",
      "3096\n"
     ]
    }
   ],
   "source": [
    "list00 = []\n",
    "for folder in data_folders[:1]:\n",
    "    files = [file for file in os.listdir(os.path.join(data_path, folder))]\n",
    "    print(files)\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_json(os.path.join(data_path, folder, file))\n",
    "        print(len(df))\n",
    "        list00.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files contained in folder 00: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files contained in folder 01: 100\n",
      "Number of files contained in folder 02: 100\n",
      "Number of files contained in folder 03: 100\n",
      "Number of files contained in folder 04: 100\n",
      "Number of files contained in folder 05: 100\n",
      "Number of files contained in folder 06: 100\n",
      "Number of files contained in folder 07: 100\n",
      "Number of files contained in folder 08: 100\n",
      "Number of files contained in folder 09: 100\n",
      "Number of files contained in folder 10: 100\n",
      "Number of files contained in folder 11: 100\n",
      "Number of files contained in folder 12: 100\n",
      "Number of files contained in folder 13: 100\n",
      "Number of files contained in folder 14: 100\n",
      "Number of files contained in folder 15: 100\n",
      "Number of files contained in folder 16: 100\n",
      "Number of files contained in folder 17: 100\n",
      "Number of files contained in folder 18: 100\n",
      "Number of files contained in folder 19: 100\n",
      "Number of files contained in folder 20: 100\n",
      "Dropped 237520 rows which contain pers.ind.articleauthor mention\n"
     ]
    }
   ],
   "source": [
    "list_all = []\n",
    "\n",
    "for folder in data_folders:\n",
    "    files = [file for file in os.listdir(os.path.join(data_path, folder))]\n",
    "    \n",
    "    print(f\"Number of files contained in folder {folder}:\", len(files))\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_json(os.path.join(data_path, folder, file))\n",
    "        list_all.append(df)\n",
    "\n",
    "mentions_all = pd.concat(list_all, axis=0, ignore_index=True)\n",
    "\n",
    "#dropping articleauthor\n",
    "len_before = len(mentions_all)\n",
    "mentions_all = mentions_all[mentions_all['entity'] != \"pers.ind.articleauthor\"]\n",
    "print(f\"Dropped {len_before - len(mentions_all)} rows which contain pers.ind.articleauthor mention\")\n",
    "\n",
    "#infer new columns\n",
    "mentions_all['split_id'] = mentions_all['id'].str.split(\"-\")\n",
    "mentions_all['article'] = mentions_all['id'].apply(lambda x: x.split(\":\")[0])\n",
    "mentions_all['newspaper'] = mentions_all['split_id'].apply(lambda x: x[0])\n",
    "mentions_all['date'] = mentions_all['split_id'].apply(lambda x: \"-\".join(x[1:4]))\n",
    "mentions_all['year'] = mentions_all['split_id'].apply(lambda x: x[1]).astype(int)\n",
    "mentions_all['decade'] = mentions_all['year'] // 10 * 10\n",
    "mentions_all['agency'] = mentions_all['entity'].str.replace(\"org.ent.pressagency.\", \"\", regex=False)\n",
    "mentions_all['language'] = mentions_all['id'].apply(lambda x: x[-2:])\n",
    "#newspapers from Switzerland are uppercase, from Luxembourg lowercase\n",
    "mentions_all['country'] = [\"CH\" if entry.isupper() else \"LU\" for entry in mentions_all['newspaper']]\n",
    "#correct typo\n",
    "#mentions_all['agency'] = mentions_all['agency'].str.replace(\"ATB\", \"ATS\", regex=False)\n",
    "mentions_all = mentions_all.drop(columns='split_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>surface</th>\n",
       "      <th>qid</th>\n",
       "      <th>lSentenceOffset</th>\n",
       "      <th>rSentenceOffset</th>\n",
       "      <th>sentence_idx:</th>\n",
       "      <th>lArticleOffset</th>\n",
       "      <th>rArticleOffset</th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>decade</th>\n",
       "      <th>agency</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.ent.pressagency.Havas</td>\n",
       "      <td>Havas</td>\n",
       "      <td>Q2826560</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>89</td>\n",
       "      <td>LLE-1939-11-27-a-i0076:1:30:35:84:89:newsag:be...</td>\n",
       "      <td>LLE-1939-11-27-a-i0076</td>\n",
       "      <td>LLE</td>\n",
       "      <td>1939-11-27</td>\n",
       "      <td>1939</td>\n",
       "      <td>1930</td>\n",
       "      <td>Havas</td>\n",
       "      <td>fr</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.ent.pressagency.UP-UPI</td>\n",
       "      <td>United Press</td>\n",
       "      <td>Q493845</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>595</td>\n",
       "      <td>607</td>\n",
       "      <td>LLE-1939-11-27-a-i0076:7:2:14:595:607:newsag:b...</td>\n",
       "      <td>LLE-1939-11-27-a-i0076</td>\n",
       "      <td>LLE</td>\n",
       "      <td>1939-11-27</td>\n",
       "      <td>1939</td>\n",
       "      <td>1930</td>\n",
       "      <td>UP-UPI</td>\n",
       "      <td>fr</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.ent.pressagency.UP-UPI</td>\n",
       "      <td>United Press</td>\n",
       "      <td>Q493845</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1364</td>\n",
       "      <td>1376</td>\n",
       "      <td>LLE-1939-11-27-a-i0076:10:0:12:1364:1376:newsa...</td>\n",
       "      <td>LLE-1939-11-27-a-i0076</td>\n",
       "      <td>LLE</td>\n",
       "      <td>1939-11-27</td>\n",
       "      <td>1939</td>\n",
       "      <td>1930</td>\n",
       "      <td>UP-UPI</td>\n",
       "      <td>fr</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.ent.pressagency.UP-UPI</td>\n",
       "      <td>United Press</td>\n",
       "      <td>Q493845</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>2181</td>\n",
       "      <td>2193</td>\n",
       "      <td>LLE-1939-11-27-a-i0076:16:2:14:2181:2193:newsa...</td>\n",
       "      <td>LLE-1939-11-27-a-i0076</td>\n",
       "      <td>LLE</td>\n",
       "      <td>1939-11-27</td>\n",
       "      <td>1939</td>\n",
       "      <td>1930</td>\n",
       "      <td>UP-UPI</td>\n",
       "      <td>fr</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.ent.pressagency.UP-UPI</td>\n",
       "      <td>Press</td>\n",
       "      <td>Q493845</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>2531</td>\n",
       "      <td>2536</td>\n",
       "      <td>LLE-1939-11-27-a-i0076:18:48:53:2531:2536:news...</td>\n",
       "      <td>LLE-1939-11-27-a-i0076</td>\n",
       "      <td>LLE</td>\n",
       "      <td>1939-11-27</td>\n",
       "      <td>1939</td>\n",
       "      <td>1930</td>\n",
       "      <td>UP-UPI</td>\n",
       "      <td>fr</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       entity       surface       qid  lSentenceOffset  \\\n",
       "0   org.ent.pressagency.Havas         Havas  Q2826560               30   \n",
       "1  org.ent.pressagency.UP-UPI  United Press   Q493845                2   \n",
       "2  org.ent.pressagency.UP-UPI  United Press   Q493845                0   \n",
       "3  org.ent.pressagency.UP-UPI  United Press   Q493845                2   \n",
       "4  org.ent.pressagency.UP-UPI         Press   Q493845               48   \n",
       "\n",
       "   rSentenceOffset  sentence_idx:  lArticleOffset  rArticleOffset  \\\n",
       "0               35              1              84              89   \n",
       "1               14              7             595             607   \n",
       "2               12             10            1364            1376   \n",
       "3               14             16            2181            2193   \n",
       "4               53             18            2531            2536   \n",
       "\n",
       "                                                  id                 article  \\\n",
       "0  LLE-1939-11-27-a-i0076:1:30:35:84:89:newsag:be...  LLE-1939-11-27-a-i0076   \n",
       "1  LLE-1939-11-27-a-i0076:7:2:14:595:607:newsag:b...  LLE-1939-11-27-a-i0076   \n",
       "2  LLE-1939-11-27-a-i0076:10:0:12:1364:1376:newsa...  LLE-1939-11-27-a-i0076   \n",
       "3  LLE-1939-11-27-a-i0076:16:2:14:2181:2193:newsa...  LLE-1939-11-27-a-i0076   \n",
       "4  LLE-1939-11-27-a-i0076:18:48:53:2531:2536:news...  LLE-1939-11-27-a-i0076   \n",
       "\n",
       "  newspaper        date  year  decade  agency language country  \n",
       "0       LLE  1939-11-27  1939    1930   Havas       fr      CH  \n",
       "1       LLE  1939-11-27  1939    1930  UP-UPI       fr      CH  \n",
       "2       LLE  1939-11-27  1939    1930  UP-UPI       fr      CH  \n",
       "3       LLE  1939-11-27  1939    1930  UP-UPI       fr      CH  \n",
       "4       LLE  1939-11-27  1939    1930  UP-UPI       fr      CH  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages in corpus: ['fr' 'de']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Languages in corpus: {mentions_all['language'].unique()}\")\n",
    "mentions_fr = mentions_all[mentions_all['language']==\"fr\"]\n",
    "mentions_de = mentions_all[mentions_all['language']==\"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = mentions_all.groupby(['article']).agg({\n",
    "    'agency': list,\n",
    "    'surface': list,\n",
    "    'id': list,\n",
    "    'sentence_idx:': list,\n",
    "    'language': 'first',\n",
    "    'newspaper': 'first',\n",
    "    'country': 'first',\n",
    "    'year': 'first',\n",
    "    'decade': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "articles_fr = articles[articles['language']==\"fr\"]\n",
    "articles_de = articles[articles['language']==\"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>agency</th>\n",
       "      <th>surface</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_idx:</th>\n",
       "      <th>language</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLB-1846-03-28-a-i0002</td>\n",
       "      <td>[Reuters]</td>\n",
       "      <td>[Reuel]</td>\n",
       "      <td>[BLB-1846-03-28-a-i0002:88:0:5:13580:13585:new...</td>\n",
       "      <td>[88]</td>\n",
       "      <td>de</td>\n",
       "      <td>BLB</td>\n",
       "      <td>CH</td>\n",
       "      <td>1846</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLB-1846-10-10-a-i0003</td>\n",
       "      <td>[Havas]</td>\n",
       "      <td>[Saas]</td>\n",
       "      <td>[BLB-1846-10-10-a-i0003:49:0:4:6901:6905:newsa...</td>\n",
       "      <td>[49]</td>\n",
       "      <td>de</td>\n",
       "      <td>BLB</td>\n",
       "      <td>CH</td>\n",
       "      <td>1846</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLB-1847-02-13-a-i0001</td>\n",
       "      <td>[unk]</td>\n",
       "      <td>[pp]</td>\n",
       "      <td>[BLB-1847-02-13-a-i0001:168:0:2:13072:13074:ne...</td>\n",
       "      <td>[168]</td>\n",
       "      <td>de</td>\n",
       "      <td>BLB</td>\n",
       "      <td>CH</td>\n",
       "      <td>1847</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLB-1847-05-22-a-i0003</td>\n",
       "      <td>[SPK-SMP]</td>\n",
       "      <td>[sp]</td>\n",
       "      <td>[BLB-1847-05-22-a-i0003:37:0:2:2653:2655:newsa...</td>\n",
       "      <td>[37]</td>\n",
       "      <td>de</td>\n",
       "      <td>BLB</td>\n",
       "      <td>CH</td>\n",
       "      <td>1847</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLB-1847-07-03-a-i0001</td>\n",
       "      <td>[SPK-SMP]</td>\n",
       "      <td>[resp]</td>\n",
       "      <td>[BLB-1847-07-03-a-i0001:11:40:44:3285:3289:new...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>de</td>\n",
       "      <td>BLB</td>\n",
       "      <td>CH</td>\n",
       "      <td>1847</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  article     agency  surface  \\\n",
       "0  BLB-1846-03-28-a-i0002  [Reuters]  [Reuel]   \n",
       "1  BLB-1846-10-10-a-i0003    [Havas]   [Saas]   \n",
       "2  BLB-1847-02-13-a-i0001      [unk]     [pp]   \n",
       "3  BLB-1847-05-22-a-i0003  [SPK-SMP]     [sp]   \n",
       "4  BLB-1847-07-03-a-i0001  [SPK-SMP]   [resp]   \n",
       "\n",
       "                                                  id sentence_idx: language  \\\n",
       "0  [BLB-1846-03-28-a-i0002:88:0:5:13580:13585:new...          [88]       de   \n",
       "1  [BLB-1846-10-10-a-i0003:49:0:4:6901:6905:newsa...          [49]       de   \n",
       "2  [BLB-1847-02-13-a-i0001:168:0:2:13072:13074:ne...         [168]       de   \n",
       "3  [BLB-1847-05-22-a-i0003:37:0:2:2653:2655:newsa...          [37]       de   \n",
       "4  [BLB-1847-07-03-a-i0001:11:40:44:3285:3289:new...          [11]       de   \n",
       "\n",
       "  newspaper country  year  decade  \n",
       "0       BLB      CH  1846    1840  \n",
       "1       BLB      CH  1846    1840  \n",
       "2       BLB      CH  1847    1840  \n",
       "3       BLB      CH  1847    1840  \n",
       "4       BLB      CH  1847    1840  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(articles_fr, output_path + \"articles_fr.pkl\")\n",
    "pd.to_pickle(articles_de, output_path + \"articles_de.pkl\")\n",
    "pd.to_pickle(mentions_fr, output_path + \"mentions_fr.pkl\")\n",
    "pd.to_pickle(mentions_de, output_path + \"mentions_de.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impresso-na-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
